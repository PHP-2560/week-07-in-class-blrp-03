---
title: "Simulations In-Class Project"
date: "Due October 13, 2017 at 11:59pm"
output:
  html_document


---

<style type="text/css">
.table {

    width: 80%;
    margin-left:10%; 
    margin-right:10%;
}
</style>
```{r,setup, echo=FALSE, cache=TRUE}
## numbers >= 10^5 will be denoted in scientific notation,
## and rounded to 2 digits
options(scipen = 3, digits = 3)
```




#Project Goals:



With this project we will simulate a famous probability problem. This will not require knowledge of probability or statistics but only the logic to follow the steps in order to simulate this problem. This is one way to solve problems by using the computer. 


Since you all have completed problem 1, you first step will be to work through each of your groupmates code for problem #1 and comment on what is happening. Then as a team move forward in on direction as you move on to the rest of the problems 2-5

 1. **Gambler's Ruin**: Suppose you have a bankroll of $1000 and make bets of $100 on a fair game. By simulating the outcome directly for at most 5000 iterations of the game (or hands), estimate:
    a. the probability that you have "busted" (lost all your money) by the time you have placed your one hundredth bet. 
    b. the probability that you have busted by the time you have placed your five hundredth bet by simulating the outcome directly. 
    c. the mean time you go bust, given that you go bust within the first 5000 hands.
    d. the mean and variance of your bankroll after 100 hands (including busts).
    e. the mean and variance of your bankroll after 500 hands (including busts).
 
Note: you *must* stop playing if your player has gone bust. How will you handle this in the `for` loop?

2. Repeat the previous problem with betting on black in American roulette, where the probability of winning on any spin is 18/38 for an even payout.

3. **Markov Chains**. Suppose you have a game where the probability of winning on your first hand is 48%; each time you win, that probability goes up by one percentage point for the next game (to a maximum of 100%, where it must stay), and each time you lose, it goes back down to 48%. Assume you cannot go bust and that the size of your wager is a constant $100.
    a. Is this a fair game? Simulate one hundred thousand sequential hands to determine the size of your return. Then repeat this simulation 99 more times to get a range of values to calculate the expectation.a. Is this a fair game? Simulate one hundred thousand sequential hands to determine the size of your return. Then repeat this simulation 99 more times to get a range of values to calculate the expectation.
```{r, cache = T}
#This function takes arguments for n many hands with a starting probability of winning of p, your bankroll, and the amount you bet each hand, and the increment of the probability increase after each win. Then it returns your bankroll at the end of the betting.
n_hands_incr <- function(n, odds, bankroll, bet, increment){
  start_p <- odds
  end_bankrolls <- c()
  for(i in 1:n){
    #Check to make sure odds dont go above 1.
    if(odds >= 1){
      odds = 1
    }
    #Simulate hand.
    win <- rbinom(1,1,odds)
    if(win == 1){
      bankroll = bankroll + bet
      #The odds change according to the increment if you win.
      odds = odds + increment
    }else{
      bankroll = bankroll - bet
      #The odds revert to the original odds if you lose.
      odds = start_p
    }
  }
  #The bankroll is returned.
  bankroll
}

hundred_tries <- replicate(100, n_hands_incr(100000, 0.48, 0, 100, 0.01))

mean(hundred_tries)
#The mean is a high negative value, so the game is rather unfair.
```
    b. Repeat this process but change the starting probability to a new value within 2% either way. Get the expected return after 100 repetitions. Keep exploring until you have a return value that is as fair as you can make it. Can you do this automatically?
```{r, cache = T}
#Test with starting probability equal to 49%
hundred_tries <- replicate(100, n_hands_incr(100000, 0.49, 0, 100, 0.01))

mean(hundred_tries)
#The value is typically a relatively small positive number, so the game is reasonably fair. Since at 48% the value is negative and at 49% the value is positive, the fairest value probably lies between 48% and 49%. Then we can test the values between 48% and 49% by a tenth of a percent each time, and find the value closest to zero. We'll also switch to 10000 hands to keep computing time down.

list_of_means <- c()
for(p in seq(from = 0.48, to = 0.49, by = 0.001)){
  hundred_tries <- replicate(100, n_hands_incr(10000, p, 0, 100, 0.01))
  list_of_means <- c(list_of_means, mean(hundred_tries))
}
list_of_means

#The mean for 48.9% is negative and for 49.0% is positive. We can take 48.85% and say that this gives a quite fair game (note that since these numbers are randomly generated it is not always the case that the means switch from positive to negative at this point). You could find an even fairer starting probability using more advanced methods.

#Test with starting probability equal to 48.95%
hundred_tries_1 <- replicate(100, n_hands_incr(100000, 0.4885, 0, 100, 0.01))

mean(hundred_tries_1)
```
    c. Repeat again, keeping the initial probability at 48%, but this time change the probability increment to a value different from 1%. Get the expected return after 100 repetitions. Keep changing this value until you have a return value that is as fair as you can make it. 
```{r, cache = T}
#Test with increment equal to 2%
hundred_tries <- replicate(100, n_hands_incr(100000, 0.48, 0,100, 0.02))

mean(hundred_tries)

#In this case, the expected value is large and positive. Remember that when the increment was 1% the value was large and negative. We'll use a similar method to the previous part to test increment values by the tenth of a percent between 1% and 2%. In this case it doesn't quite make sense to change the number of hands to 10000 rather than 100000, since the higher increments means there is a better chance that the probability will get to 1, caussing a long series of wins. Then changing the number of hands will drastically change the returns of the game.

list_of_means <- c()
for(p in seq(from = 0.01, to = 0.02, by = 0.001)){
  hundred_tries <- replicate(100, n_hands_incr(100000, 0.48, 0, 100, p))
  list_of_means <- c(list_of_means, mean(hundred_tries))
}
list_of_means

#The means change from positive to negative between 1.2% and 1.3% (again this may not always be the case). We'll split the difference at 1.25% to get a reasonably fair game. Again similar methods can help make the game fairer if you want.

hundred_tries_2 <- replicate(100, n_hands_incr(100000, 0.48, 0,100, 0.0135))

mean(hundred_tries_2)
```


4. Creating a Bootstrap function. There is a particular concept called [bootstrapping]
(https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) where we can easily create 95% confidence intervals, even for complex estimators.

The steps of this process are:

  a. Draw a sample, with replacement, from your data which is the same length of your data.
  b. Calculate the statistic of interest on this boostrap sample (ie mean, variance, regression,...)
  c. Peform steps 1:2 at least 1000 times over until you have a vector of your statistics. 
  d. The lower bound of a 95% CI will be the 0.025 percentile
  e. The upper bound of a 95% CI will be the 0.975 percentile

Make a function called `boot_ci` which calculates the 95% confidence interval in this manner.

```{r}
#This bootstrap function takes a vector of data, as well as some function called statistic which is the function of the data you are interested in.
boot_ci <- function(data, statistic){
  n <- length(data)
  #This vector will hold the values of the function statistic applied to the random sample with replacement from the data.
  samples <- c()
  #The process is run 5000 times in this case.
  for(i in 1:5000){
    #n random numbers from 1 to n are chosen
    random_sample <- sample(1:n,n,replace = T)
    #The random numbers that were chosen are the indices of the given data, this gives a sample of the data with the same length, with replacement.
    sample <- data[random_sample]
    #The desired statistic is then applied to the data, this is added to the vector samples.
    samples[i] <- statistic(sample)
  }
  #The quantile function on samples gives an estimate of the 95% confidence interval, which is returned.
  quantile(samples,c(0.025, 0.975))
}
```

5. For problems 3b and 3c, you calculated a mean value. Because you saved these final results in a vector, use the bootstrap to estimate the variance of the return in each case for your final answer. Once you have these results, which game has the smaller variance in returns?

```{r}
#The collection of means from parts 3b and 3c are saved as hundred_means_1 and hundred_means_2 respectively. Then we can call the new boot_ci function on these vectors with statistic equal to the variance function.

boot_ci(hundred_tries_1, var)

boot_ci(hundred_tries_2, var)
```